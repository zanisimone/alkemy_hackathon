{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import chi2_contingency\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/main-path...'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "def load_datasets(dataset_names, base_path):\n",
    "    datasets = {}\n",
    "    for name in dataset_names:\n",
    "        file_path = os.path.join(base_path, name + '.csv')\n",
    "        try:\n",
    "            datasets[name] = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            datasets[name] = None\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/base-path...'\n",
    "\n",
    "# Dataset names: assuming you have mulpiple dataset to merge\n",
    "dataset_names = ['datasets-names-list']\n",
    "\n",
    "# Assuming you have a function load_datasets that loads the datasets given the names and path\n",
    "datasets = load_datasets(dataset_names, base_path)\n",
    "\n",
    "# Displaying the first few rows of each dataset to understand their structure\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\nFirst few rows of {name}:\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(f\"Failed to load {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below only if you need to merge multiple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_datasets(datasets):\n",
    "    # Start by merging orders with customers\n",
    "    merged = pd.merge(datasets['olist_orders_dataset'], datasets['olist_customers_dataset'], on='customer_id', how='left')\n",
    "\n",
    "    # Add other datasets with the correct merge keys\n",
    "    # i.e. 'order_items': 'order_id', 'order_payments': 'order_id'\n",
    "    merge_keys = {'name':'key'}\n",
    "\n",
    "    for name, key in merge_keys.items():\n",
    "        if name in datasets:\n",
    "            merged = pd.merge(merged, datasets[name], on=key, how='left')\n",
    "\n",
    "    return merged\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load individual datasets\n",
    "# i.e. olist_customers_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv')\n",
    "\n",
    "\n",
    "# Define the datasets dictionary: associate name to df object\n",
    "# i.e. 'olist_customers_dataset': olist_customers_df,\n",
    "datasets = { 'name':'key'}\n",
    "\n",
    "# Now, you can call the merge_datasets function with the loaded datasets\n",
    "merged_df = merge_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values by percentage in each column\n",
    "merged_df.isnull().sum() / len(merged_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values column with more than 50% missing values\n",
    "merged_df = merged_df.dropna(thresh=len(merged_df) * 0.5, axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values by percentage in each column\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Drop columns with more than 50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "    \n",
    "    # Convert datetime columns: only if you have column with date and time values\n",
    "    datetime_cols = ['columns to convert']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate new features\n",
    "    # i.e. df['time_to_delivery'] = (df['order_delivered_customer_date'] - df['order_approved_at']).dt.days\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # create seasonal features from order_purchase_timestamp\n",
    "    # i.e. df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "\n",
    "    return df\n",
    "\n",
    "merged_df = preprocess_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "merged_df.drop(['columns names'], axis=1, inplace=True) \n",
    "# save the cleaned dataset\n",
    "merged_df.to_csv('ds_merged_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary statistics\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for numerical values (Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "# Plot the heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix (Pearson)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for categorical values (Cramer's V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cramér's V\n",
    "def cramers_v(x, y):\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    \n",
    "    # Perform Chi-Square test\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Calculate Cramér's V\n",
    "    n = contingency_table.sum().sum()\n",
    "    r, k = contingency_table.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r-1, k-1))))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['list of categorical column']\n",
    "\n",
    "# Create an empty matrix to store Cramér's V values\n",
    "n = len(categorical_columns)\n",
    "cramers_v_matrix = pd.DataFrame(np.zeros((n, n)), index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "# Calculate Cramér's V for each pair of variables\n",
    "for col1 in categorical_columns:\n",
    "    for col2 in categorical_columns:\n",
    "        cramers_v_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5)\n",
    "plt.title(\"Cramér's V Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df, columns_to_scale): \n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Apply the Min-Max scaling (normalization) to the dataset\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns_to_scale)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to scale\n",
    "columns_to_scale = ['list of columns']\n",
    "\n",
    "# Scale Data\n",
    "df_scaled = data_scaler(df, columns_to_scale)\n",
    "\n",
    "# Display the normalized dataset\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nNormalized Data:\\n\", df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract classification features from feature sets, and \n",
    "save thes6 feature sets in a new table file for further\r\n",
    "analysis. Many feature labels about date and classificati n\r\n",
    "that have no obvious correlation w ud\r\n",
    "predic \n",
    "labeltlts are excluded from the total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL SCREENING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance test (numerical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VarianceThreshold with a threshold of 0.062\n",
    "selector = VarianceThreshold(threshold=0.062)\n",
    "selected_features = selector.fit_transform(df)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_columns = df.columns[selector.get_support()]\n",
    "\n",
    "# Create a new DataFrame with selected features\n",
    "df_selected = pd.DataFrame(selected_features, columns=selected_columns)\n",
    "\n",
    "# Display the selected features\n",
    "print(\"Selected Features Based on Variance Threshold (0.062):\")\n",
    "print(df_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test (categorical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Define the target variable\n",
    "target = 'class'\n",
    "\n",
    "# List to store Chi-Square values\n",
    "chi2_values = []\n",
    "features = []\n",
    "\n",
    "# Iterate over categorical columns (excluding the target)\n",
    "for column in df.columns:\n",
    "    if column != target:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df[column], df[target])\n",
    "        \n",
    "        # Perform the Chi-Square test\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "        # Store the Chi-Square value and feature name\n",
    "        chi2_values.append(chi2)\n",
    "        features.append(column)\n",
    "\n",
    "        # Display the results for each categorical variable\n",
    "        print(f\"\\nChi-Square Test Results for '{column}':\")\n",
    "        print(f\"Contingency Table:\\n{contingency_table}\")\n",
    "        print(f\"Chi-Square Statistic: {chi2}\")\n",
    "        print(f\"P-Value: {p}\")\n",
    "        print(f\"Degrees of Freedom: {dof}\")\n",
    "        print(\"Expected Frequencies:\\n\", expected)\n",
    "\n",
    "# Plotting the Chi-Square values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(features, chi2_values, color='skyblue')\n",
    "plt.title('Chi-Square Values for Categorical Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Chi-Square Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05  # Significance level\n",
    "for i, p in enumerate([stats.chi2_contingency(pd.crosstab(df[column], df[target]))[1] for column in features]):\n",
    "    if p < alpha:\n",
    "        print(f\"\\nReject the null hypothesis for '{features[i]}': There is a significant association.\")\n",
    "    else:\n",
    "        print(f\"\\nFail to reject the null hypothesis for '{features[i]}': There is no significant association.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECONDARY SCREENING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoder (code written using paper column name, only for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Fit and transform the categorical variables\n",
    "encoded_features = encoder.fit_transform(df[['incident_type', 'collision_type']])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['incident_type', 'collision_type']))\n",
    "\n",
    "# Concatenate the original numerical data with the one-hot encoded DataFrame\n",
    "df_encoded = pd.concat([df.drop(['incident_type', 'collision_type'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display the DataFrame after one-hot encoding\n",
    "print(\"\\nDataFrame after One-Hot Encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Scores\n",
    "According to the data, the label \n",
    "with small scores will be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_score(df, feature, target):\n",
    "    classes = df[target].unique()\n",
    "    mean = []\n",
    "    variance = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        mean.append(df[df[target] == cls][feature].mean())\n",
    "        variance.append(df[df[target] == cls][feature].var(ddof=0))  # Population variance\n",
    "    \n",
    "    # Fisher score calculation\n",
    "    fisher_score = (mean[0] - mean[1])**2 / (variance[0] + variance[1])\n",
    "    return fisher_score\n",
    "\n",
    "# Calculate Fisher scores for each feature\n",
    "fisher_scores = {}\n",
    "for feature in df.columns[:-1]:  # Exclude the target column\n",
    "    fisher_scores[feature] = fisher_score(df, feature, 'class')\n",
    "\n",
    "# Create a DataFrame to display Fisher scores\n",
    "fisher_scores_df = pd.DataFrame(list(fisher_scores.items()), columns=['Feature', 'Fisher Score'])\n",
    "\n",
    "# Display the Fisher scores\n",
    "print(\"Fisher Scores for Customer Features:\")\n",
    "print(fisher_scores_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 55151,
     "sourceId": 2669146,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4607364,
     "sourceId": 7855431,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
