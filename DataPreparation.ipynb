{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import chi2_contingency\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/main-path...'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "def load_datasets(dataset_names, base_path):\n",
    "    datasets = {}\n",
    "    for name in dataset_names:\n",
    "        file_path = os.path.join(base_path, name + '.csv')\n",
    "        try:\n",
    "            datasets[name] = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            datasets[name] = None\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/base-path...'\n",
    "\n",
    "# Dataset names: assuming you have mulpiple dataset to merge\n",
    "dataset_names = ['datasets-names-list']\n",
    "\n",
    "# Assuming you have a function load_datasets that loads the datasets given the names and path\n",
    "datasets = load_datasets(dataset_names, base_path)\n",
    "\n",
    "# Displaying the first few rows of each dataset to understand their structure\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\nFirst few rows of {name}:\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(f\"Failed to load {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below only if you need to merge multiple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_datasets(datasets):\n",
    "    # Start by merging orders with customers\n",
    "    merged = pd.merge(datasets['olist_orders_dataset'], datasets['olist_customers_dataset'], on='customer_id', how='left')\n",
    "\n",
    "    # Add other datasets with the correct merge keys\n",
    "    # i.e. 'order_items': 'order_id', 'order_payments': 'order_id'\n",
    "    merge_keys = {'name':'key'}\n",
    "\n",
    "    for name, key in merge_keys.items():\n",
    "        if name in datasets:\n",
    "            merged = pd.merge(merged, datasets[name], on=key, how='left')\n",
    "\n",
    "    return merged\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load individual datasets\n",
    "# i.e. olist_customers_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv')\n",
    "\n",
    "\n",
    "# Define the datasets dictionary: associate name to df object\n",
    "# i.e. 'olist_customers_dataset': olist_customers_df,\n",
    "datasets = { 'name':'key'}\n",
    "\n",
    "# Now, you can call the merge_datasets function with the loaded datasets\n",
    "merged_df = merge_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values by percentage in each column\n",
    "merged_df.isnull().sum() / len(merged_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values column with more than 50% missing values\n",
    "merged_df = merged_df.dropna(thresh=len(merged_df) * 0.5, axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values by percentage in each column\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Drop columns with more than 50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "    \n",
    "    # Convert datetime columns: only if you have column with date and time values\n",
    "    datetime_cols = ['columns to convert']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate new features\n",
    "    # i.e. df['time_to_delivery'] = (df['order_delivered_customer_date'] - df['order_approved_at']).dt.days\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # create seasonal features from order_purchase_timestamp\n",
    "    # i.e. df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "\n",
    "    return df\n",
    "\n",
    "merged_df = preprocess_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "merged_df.drop(['columns names'], axis=1, inplace=True) \n",
    "# save the cleaned dataset\n",
    "merged_df.to_csv('ds_merged_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary statistics\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation for numerical values (Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "# Plot the heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix (Pearson)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation for categorical values (Cramer's V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cramér's V\n",
    "def cramers_v(x, y):\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    \n",
    "    # Perform Chi-Square test\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Calculate Cramér's V\n",
    "    n = contingency_table.sum().sum()\n",
    "    r, k = contingency_table.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r-1, k-1))))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['list of categorical column']\n",
    "\n",
    "# Create an empty matrix to store Cramér's V values\n",
    "n = len(categorical_columns)\n",
    "cramers_v_matrix = pd.DataFrame(np.zeros((n, n)), index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "# Calculate Cramér's V for each pair of variables\n",
    "for col1 in categorical_columns:\n",
    "    for col2 in categorical_columns:\n",
    "        cramers_v_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5)\n",
    "plt.title(\"Cramér's V Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df, columns_to_scale): \n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Apply the Min-Max scaling (normalization) to the dataset\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns_to_scale)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Select columns to scale\n",
    "columns_to_scale = ['list of columns']\n",
    "\n",
    "# Scale Data\n",
    "df_scaled = data_scaler(df, columns_to_scale)\n",
    "\n",
    "# Display the normalized dataset\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nNormalized Data:\\n\", df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract classification features from feature sets, and \n",
    "save thes6 feature sets in a new table file for further\r\n",
    "analysis. Many feature labels about date and classificati n\r\n",
    "that have no obvious correlation w ud\r\n",
    "predic \n",
    "labeltlts are excluded from the total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL SCREENING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECONDARY SCREENING"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 55151,
     "sourceId": 2669146,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4607364,
     "sourceId": 7855431,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
